{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mmObVNuEVJQ"
      },
      "source": [
        "# Experiment Architecture\n",
        "## Summarization + RAG (Retrieval Augmented Generation)\n",
        "#### Differences\n",
        "- Delta is not used for data operations in this experiment\n",
        "- The input data are HTML not XML files\n",
        "#### Problem Statement\n",
        "- We want to get Dolly knowledgeable about the Amadeus open source DataIO framework\n",
        "#### Flow\n",
        "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/product/llm-dolly/llm-dolly-full.png\" width=\"1000\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPqureHXDs_T"
      },
      "source": [
        "# Reading HTML pages from the DataIO documentation\n",
        "<img src=\"https://cdn-icons-png.flaticon.com/512/4248/4248142.png\" width=\"250\" />\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA1yDXCFNmM5",
        "outputId": "aabead8a-42d8-4c96-91d8-021d726eaae1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:08<00:00,  1.76it/s]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "\n",
        "def slice_html_page(html_page):\n",
        "  chunks = re.findall(r\"(<.+>)+\", html_page)\n",
        "  merged_chunks = []\n",
        "  # split in chunks with no overlap\n",
        "  for i in range(len(chunks), 50):\n",
        "    merged_chunks.append(chunks[i:i+50])\n",
        "  # append the last part, in case len(chunks)%50!=0\n",
        "  # the possible overalp of the last 2 chunks is not an issue for the LLM\n",
        "  merged_chunks.append(chunks[-50:])\n",
        "  return merged_chunks\n",
        "\n",
        "\n",
        "page_list = [\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/getting-started.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/main-concepts.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/writing-processors.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/configuration.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/processors.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/schema-definitions.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/paths.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/date-filters.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/pipes/pipes.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/pipes/storage.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/pipes/elk.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/pipes/kafka.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/distributors/distributors.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/distributors/email.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/advanced/custom-processor-traits.html\"\n",
        "]\n",
        "\n",
        "html_page_list = []\n",
        "\n",
        "for page_link in tqdm(page_list):\n",
        "  response = requests.get(page_link)\n",
        "  html_doc = response.text\n",
        "  soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "  str_html = soup.prettify()\n",
        "  html_page_list.append(slice_html_page(str_html))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G2NrLY_EPQm"
      },
      "source": [
        "# Embeddings\n",
        "##### Multidimensional Vector Representation of Semantic Meanings\n",
        "<img src=\"https://corpling.hypotheses.org/files/2018/04/Screen-Shot-2018-04-25-at-13.21.44.png\" width=\"400\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ltUAoTN84hCp"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import itertools\n",
        "\n",
        "model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
        "\n",
        "model = SentenceTransformer(model_name, device='cuda')\n",
        "\n",
        "sentences = [[\"\".join(chunks) for chunks in chunk_list] for chunk_list in html_page_list]\n",
        "\n",
        "flattened_sentences = sentences = list(itertools.chain(*sentences))\n",
        "\n",
        "embeddings = [[float(x) for x in model.encode(s)] for s in sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfpMfNasEejg"
      },
      "source": [
        "# Database\n",
        "##### Creating a Vector Database of Embeddings using the open-source ChromaDB\n",
        "<img src=\"https://www.mlq.ai/content/images/2023/08/1_admwyPyR6v_IZI0EYE--eA-1.webp\" width=\"250\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz9RWEbs6sTV"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
        "\n",
        "collection = chroma_client.get_or_create_collection(name=\"leader_election_distributed_systems\", embedding_function=default_ef)\n",
        "\n",
        "collection.add(\n",
        "    documents=html_page_list,\n",
        "    embeddings=embeddings,\n",
        "    ids=[str(x) for x in range(len(embeddings))]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01u1A_UhEhUm"
      },
      "source": [
        "# ChatBot\n",
        "##### Using the open-source Databricks' Dolly model fine-tuned through the RAG techinique\n",
        "<img src=\"https://www.databricks.com/sites/default/files/2023-04/Dolly-logo.png\" width=\"300\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RJ4pOpUo7S_H"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from langchain import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "\n",
        "def build_qa_chain():\n",
        "\n",
        "    model_name = \"databricks/dolly-v2-3b\" # Dolly smallest version (3 billion params)\n",
        "\n",
        "    instruct_pipeline = pipeline(model=model_name, torch_dtype=torch.bfloat16, trust_remote_code=True,\n",
        "                                 return_full_text=True, max_new_tokens=4096, top_p=0.95, top_k=50,\n",
        "                                 device=0) #cuda\n",
        "\n",
        "    template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "    Instruction:\n",
        "    You are an expert the framework DataIO.\n",
        "    You use a simple language to explain concepts.\n",
        "    You reply using textual descriptions and examples.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Response:\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(input_variables=['context', 'question'], template=template)\n",
        "\n",
        "    hf_pipe = HuggingFacePipeline(pipeline=instruct_pipeline)\n",
        "\n",
        "    return load_qa_chain(llm=hf_pipe, chain_type=\"stuff\", prompt=prompt, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZtL_eW-E7sDr"
      },
      "outputs": [],
      "source": [
        "# Building the chain will load Dolly and can take several minutes depending on the model size\n",
        "qa_chain = build_qa_chain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bZznuQYL75zj"
      },
      "outputs": [],
      "source": [
        "class Document():\n",
        "    def __init__(self, content):\n",
        "        self.page_content = content\n",
        "        self.metadata = {\"metadata\": \"\"}\n",
        "\n",
        "def get_similar_docs(question):\n",
        "    results = collection.query(\n",
        "        query_embeddings=[float(x) for x in model.encode(question)],\n",
        "        n_results=2\n",
        "    )\n",
        "    return results[\"documents\"]\n",
        "\n",
        "def answer_question(question):\n",
        "    similar_docs = [Document(x) for x in get_similar_docs(question)]\n",
        "    result = qa_chain({\"input_documents\": similar_docs, \"question\": question})\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLDLROSs795J",
        "outputId": "d86a23a7-90d5-4921-ab66-ffcb87d615f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert the framework DataIO.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using textual descriptions and examples.\n",
            "\n",
            "    [None, None]\n",
            "\n",
            "    Question: Can you shortly describe me Data IO framework?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Data IO is a multi-paradigm programming concept that is a general abstraction over a network that allows for easy and efficient data movement, processing and storage. Its core components are: \n",
            "\n",
            "- Channels - which provide a message interface for writing and reading data\n",
            "- Sinks - which read from channels and perform some form of conversion or mapping of the data they read into a different format before writing it to a channel\n",
            "- Sources - which write to channels and perform some form of conversion or mapping of the data written to it into a different format before reading it\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CURL_CA_BUNDLE'] = ''\n",
        "\n",
        "question = \"Can you shortly describe me Data IO framework?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwqQYJ5D8Gvn",
        "outputId": "bc863660-808a-4db3-e3d2-b61862172d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert the framework DataIO.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using textual descriptions and examples.\n",
            "\n",
            "    [None, None]\n",
            "\n",
            "    Question: How can I export streaming live data to Elasticsearch with DataIO?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "First, define the format for the data to be exported and the destination.\n",
            "Second, prepare the source data in this format.\n",
            "Third, export the data via the specified format using the DataIO library.\n",
            "Fourth, prepare the destination in the format required by the search engine.\n",
            "Fifth, index the data in Elasticsearch using the API.\n"
          ]
        }
      ],
      "source": [
        "question = \"How can I export streaming live data to Elasticsearch with DataIO?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UjxQz3W8NYz",
        "outputId": "9a97afad-6c8b-401a-9bac-419b81de095e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert the framework DataIO.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using textual descriptions and examples.\n",
            "\n",
            "    [None, None]\n",
            "\n",
            "    Question: Can you provide me an example of DataIO configuration file for reading from Kafka?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "DataIO is an open source library for Java to read from Apache Kafka topics.\n",
            "The configuration file for reading from Kafka is simple and looks like this:\n",
            "\n",
            "```\n",
            "dataSource {\n",
            "\turl(\"jdbc:mysql://mysql1:3306/test?useTimeZone=true&useSSL=false\")\n",
            "}\n",
            "\n",
            "tasks {\n",
            "\ttask(\"selectFromKafka\") {\n",
            "\t\t...\n",
            "\t\t...\n",
            "\t\t...\n",
            "\t\tconnect(url + \"?\", new ConnectionSettings() {\n",
            "\t\t\t...\n",
            "\t\t\t...\n",
            "\t\t\t...\n",
            "\t\t\tdriverClassName=\"com.mysql.cj.jdbc.Driver\"\n",
            "\t\t}\n",
            "\t\t)\n",
            "\t}\n",
            "}\n",
            "\n",
            "\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "question = \"Can you provide me an example of DataIO configuration file for reading from Kafka?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
