{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment Architecture\n",
        "## Summarization + RAG (Retrieval Augmented Generation)\n",
        "#### Differences\n",
        "- Delta is not used for data operations in this experiment\n",
        "- The input data are HTML not XML files\n",
        "#### Problem Statement\n",
        "- We want to get Dolly knowledgeable about the Amadeus open source DataIO framework\n",
        "#### Flow\n",
        "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/product/llm-dolly/llm-dolly-full.png\" width=\"1000\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reading HTML pages from the DataIO documentation\n",
        "<img src=\"https://cdn-icons-png.flaticon.com/512/4248/4248142.png\" width=\"250\" />\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SarOG3s4IJXq",
        "outputId": "45250578-a966-4ec9-e6e8-60bf008721b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:06<00:00,  2.39it/s]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "\n",
        "def slice_html_page(html_page):\n",
        "  chunks = re.findall(r\"(<.+>)+\", html_page)\n",
        "  merged_chunks = []\n",
        "  # split in chunks with no overlap\n",
        "  for i in range(len(chunks), 50):\n",
        "    merged_chunks.append(chunks[i:i+50])\n",
        "  # append the last part, in case len(chunks)%50!=0\n",
        "  # the possible overalp of the last 2 chunks is not an issue for the LLM\n",
        "  merged_chunks.append(chunks[-50:])\n",
        "  return merged_chunks\n",
        "\n",
        "\n",
        "page_list = [\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/getting-started.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/main-concepts.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/writing-processors.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/configuration.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/processors.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/schema-definitions.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/paths.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/date-filters.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/pipes/pipes.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/pipes/storage.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/pipes/elk.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/pipes/kafka.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/distributors/distributors.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/distributors/email.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/advanced/custom-processor-traits.html\"\n",
        "]\n",
        "\n",
        "html_page_list = []\n",
        "\n",
        "for page_link in tqdm(page_list):\n",
        "  response = requests.get(page_link)\n",
        "  html_doc = response.text\n",
        "  soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "  str_html = soup.prettify()\n",
        "  html_page_list.append(slice_html_page(str_html))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embeddings\n",
        "##### Multidimensional Vector Representation of Semantic Meanings\n",
        "<img src=\"https://corpling.hypotheses.org/files/2018/04/Screen-Shot-2018-04-25-at-13.21.44.png\" width=\"400\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SoXFMCTMINRU"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import itertools\n",
        "\n",
        "model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
        "\n",
        "model = SentenceTransformer(model_name, device='cuda')\n",
        "\n",
        "sentences = [[\"\".join(chunks) for chunks in chunk_list] for chunk_list in html_page_list]\n",
        "\n",
        "flattened_sentences = sentences = list(itertools.chain(*sentences))\n",
        "\n",
        "embeddings = [[float(x) for x in model.encode(s)] for s in sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Database\n",
        "##### Creating a Vector Database of Embeddings using the open-source ChromaDB\n",
        "<img src=\"https://www.mlq.ai/content/images/2023/08/1_admwyPyR6v_IZI0EYE--eA-1.webp\" width=\"250\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6xAmIFqIOvp",
        "outputId": "a5411eb3-85ff-446f-9dbc-623349c70f2b"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
        "\n",
        "collection = chroma_client.get_or_create_collection(name=\"data_io_doc_chunks\", embedding_function=default_ef)\n",
        "\n",
        "collection.add(\n",
        "    documents=html_page_list,\n",
        "    embeddings=embeddings,\n",
        "    ids=[str(x) for x in range(len(embeddings))]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ChatBot\n",
        "##### Using the open-source Databricks' Dolly model fine-tuned through the RAG techinique\n",
        "<img src=\"https://www.databricks.com/sites/default/files/2023-04/Dolly-logo.png\" width=\"300\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Cz8bamriIQIv"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from langchain import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "\n",
        "def build_qa_chain():\n",
        "\n",
        "    model_name = \"databricks/dolly-v2-3b\" # Dolly smallest version (3 billion params)\n",
        "\n",
        "    instruct_pipeline = pipeline(model=model_name, torch_dtype=torch.bfloat16, trust_remote_code=True,\n",
        "                                 return_full_text=True, max_new_tokens=4096, top_p=0.95, top_k=50,\n",
        "                                 device=0) #cuda\n",
        "\n",
        "    template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "    Instruction:\n",
        "    You are an expert the Amadeus DataIO Framework.\n",
        "    You use a simple language to explain concepts.\n",
        "    You reply using textual descriptions and examples take from the Amadeus DataIO documentation pages.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Response:\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(input_variables=['context', 'question'], template=template)\n",
        "\n",
        "    hf_pipe = HuggingFacePipeline(pipeline=instruct_pipeline)\n",
        "\n",
        "    return load_qa_chain(llm=hf_pipe, chain_type=\"stuff\", prompt=prompt, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "N0mT_6gNIRfZ"
      },
      "outputs": [],
      "source": [
        "# Building the chain will load Dolly and can take several minutes depending on the model size\n",
        "qa_chain = build_qa_chain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "TxKkDdXmITTB"
      },
      "outputs": [],
      "source": [
        "class Document():\n",
        "    def __init__(self, content):\n",
        "        self.page_content = content\n",
        "        self.metadata = {\"metadata\": \"DataIO documentation\"}\n",
        "\n",
        "def get_similar_docs(question):\n",
        "    results = collection.query(\n",
        "        query_embeddings=[float(x) for x in model.encode(question)],\n",
        "        n_results=1\n",
        "    )\n",
        "    return results[\"documents\"]\n",
        "\n",
        "def answer_question(question):\n",
        "    similar_docs = [Document(x) for x in get_similar_docs(question)]\n",
        "    result = qa_chain({\"input_documents\": similar_docs, \"question\": question})\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wtvZGhyIetn",
        "outputId": "e8efe4fe-ca97-4fe4-fd28-2c6cf1859c74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert the Amadeus DataIO Framework.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using textual descriptions and examples take from the Amadeus DataIO documentation pages.\n",
            "\n",
            "    [None]\n",
            "\n",
            "    Question: What is Amadeus' DataIO framework?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "The Amadeus DataIO framework is a set of technologies to collect, store, analyse and use data in the workplace. It consists of the following parts.\n",
            "1. The Data IO framework and it's components\n",
            "2. The APIs that use the Data IO framework\n",
            "3. Processes that use the APIs\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CURL_CA_BUNDLE'] = ''\n",
        "\n",
        "question = \"What is Amadeus' DataIO framework?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYWj6aijIf_h",
        "outputId": "8dc231c5-748d-4195-8f4e-ad03dd373d95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert the Amadeus DataIO Framework.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using textual descriptions and examples take from the Amadeus DataIO documentation pages.\n",
            "\n",
            "    [None]\n",
            "\n",
            "    Question: Can I use Amadeus' DataIO to read from Kafka?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "You can use Amadeus' DataIO to read from Kafka, if you use the Kafka API in your Kafka setup and provide the configuration needed to connect to Kafka from the DataIO framework, you can read from Kafka.\n"
          ]
        }
      ],
      "source": [
        "question = \"Can I use Amadeus' DataIO to read from Kafka?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZCpwyvbIiFK",
        "outputId": "b4c14df4-056a-4cca-effe-a944f1d40bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert the Amadeus DataIO Framework.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using textual descriptions and examples take from the Amadeus DataIO documentation pages.\n",
            "\n",
            "    [None]\n",
            "\n",
            "    Question: Can you provide a configuration file for reading from Kafka Input in text format?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Sure, here is a sample configuration file for a Kafka Input:\n",
            "\n",
            "[Kafka Input]\n",
            "Topic = your_kafka_topic\n",
            "Broker = your_kafka_broker_id\n",
            "GroupBy = customer,product\n",
            "FieldName = your_field_name\n",
            "FieldType = string\n",
            "DataFormat = json\n",
            "DataEncoding = UTF8\n",
            "HeaderEncoding = UTF8\n",
            "Validation = true\n",
            "FieldChecking = true\n",
            "RecordChecking = true\n",
            "MaxFieldLength = 256\n",
            "MaxBytes = 4194304\n",
            "MinFieldLength = 1\n",
            "MinBytes = 1\n"
          ]
        }
      ],
      "source": [
        "question = \"Can you provide a configuration file for reading from Kafka Input in text format?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
