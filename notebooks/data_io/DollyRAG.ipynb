{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJlAS8-NPL0V"
      },
      "source": [
        "# Experiment Architecture\n",
        "## RAG (Retrieval Augmented Generation)\n",
        "#### Differences\n",
        "- Delta is not used for data operations in this experiment\n",
        "- The input data are PDF not XML files\n",
        "#### Problem Statement\n",
        "- We want to get Dolly knowledgeable about Amadeus' Data I/0 framework\n",
        "#### Flow\n",
        "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/product/llm-dolly/llm-dolly-full.png\" width=\"1000\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l84iNWW2PMEv"
      },
      "source": [
        "# Transforming in PDF format the DataIO official Documentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeaLkJis31Sl"
      },
      "outputs": [],
      "source": [
        "from xhtml2pdf import pisa\n",
        "import requests\n",
        "\n",
        "\n",
        "def convert_url_to_pdf(url, pdf_path):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch URL: {url}\")\n",
        "        return False\n",
        "\n",
        "    html_content = response.text\n",
        "\n",
        "    with open(pdf_path, \"wb\") as pdf_file:\n",
        "        pisa_status = pisa.CreatePDF(html_content, dest=pdf_file)\n",
        "\n",
        "    return not pisa_status.err\n",
        "\n",
        "# URL to fetch\n",
        "urls_to_fetch = [\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/getting-started.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/main-concepts.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/writing-processors.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/configuration.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/processors.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/schema-definitions.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/paths.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/date-filters.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/pipes/pipes.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/pipes/storage.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/pipes/elk.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/pipes/kafka.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/distributors/distributors.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/configuration/distributors/email.html\",\n",
        "    \"https://amadeusitgroup.github.io/dataio-framework/advanced/custom-processor-traits.html\"\n",
        "]\n",
        "\n",
        "\n",
        "for i in range(len(urls_to_fetch)):\n",
        "  convert_url_to_pdf(urls_to_fetch[i], f\"dataio_{i}.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehBh4ana6s0c"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "bodies = []\n",
        "\n",
        "for i in range(len(urls_to_fetch)):\n",
        "  with open(f\"/content/dataio_{i}.pdf\", \"rb\") as file:\n",
        "      reader = PyPDF2.PdfReader(file)\n",
        "      body = []\n",
        "      for page in tqdm(reader.pages):\n",
        "          body.append(page.extract_text())\n",
        "      bodies.append(body)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH3-P5BIPjKA"
      },
      "source": [
        "# Embeddings\n",
        "##### Multidimensional Vector Representation of Semantic Meanings\n",
        "<img src=\"https://corpling.hypotheses.org/files/2018/04/Screen-Shot-2018-04-25-at-13.21.44.png\" width=\"400\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "KOj_wrqA-fEq"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import itertools\n",
        "\n",
        "\n",
        "model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
        "\n",
        "model = SentenceTransformer(model_name, device='cuda')\n",
        "\n",
        "sentences = list(itertools.chain(*bodies))\n",
        "\n",
        "embeddings = [[float(x) for x in model.encode(s)] for s in sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru0jmcrZPlXa"
      },
      "source": [
        "# Database\n",
        "##### Creating a Vector Database of Embeddings using the open-source ChromaDB\n",
        "<img src=\"https://www.mlq.ai/content/images/2023/08/1_admwyPyR6v_IZI0EYE--eA-1.webp\" width=\"250\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "gQHWawmY_X25"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
        "\n",
        "collection = chroma_client.get_or_create_collection(name=\"dataio_official_documentation\", embedding_function=default_ef)\n",
        "\n",
        "collection.add(\n",
        "    documents=sentences,\n",
        "    embeddings=embeddings,\n",
        "    ids=[str(x) for x in range(len(embeddings))]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyOnX0OQPo3l"
      },
      "source": [
        "# Fine-Tuned Model\n",
        "##### Based on the open-source Databricks' Dolly\n",
        "<img src=\"https://www.databricks.com/sites/default/files/2023-04/Dolly-logo.png\" width=\"300\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "FRHamrJfAGaW"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from langchain import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "\n",
        "def build_qa_chain():\n",
        "\n",
        "    model_name = \"databricks/dolly-v2-3b\" # Dolly smallest version (3 billion params)\n",
        "\n",
        "    instruct_pipeline = pipeline(model=model_name, torch_dtype=torch.bfloat16, trust_remote_code=True,\n",
        "                                 return_full_text=True, max_new_tokens=4096, top_p=0.95, top_k=50,\n",
        "                                 device='cuda')\n",
        "\n",
        "    template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "    Instruction:\n",
        "    You are an expert about the Amadeus' Data I/O framework.\n",
        "    You use a simple language to explain concepts.\n",
        "    You reply using only textual descriptions and code examples taken from Data I/O documentation.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Response:\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(input_variables=['context', 'question'], template=template)\n",
        "\n",
        "    hf_pipe = HuggingFacePipeline(pipeline=instruct_pipeline)\n",
        "\n",
        "    return load_qa_chain(llm=hf_pipe, chain_type=\"stuff\", prompt=prompt, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "j53AZjM9ATuS"
      },
      "outputs": [],
      "source": [
        "# Building the chain will load Dolly and can take several minutes depending on the model size\n",
        "qa_chain = build_qa_chain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "XJ0nJpsOAV4u"
      },
      "outputs": [],
      "source": [
        "class Document():\n",
        "    def __init__(self, content):\n",
        "        self.page_content = content\n",
        "        self.metadata = {\"metadata\": \"Data I/O documentation\"}\n",
        "\n",
        "def get_similar_docs(question):\n",
        "    results = collection.query(\n",
        "        query_embeddings=[float(x) for x in model.encode(question)],\n",
        "        n_results=1\n",
        "    )\n",
        "    return results[\"documents\"]\n",
        "\n",
        "def answer_question(question):\n",
        "    similar_docs = [Document(x) for x in get_similar_docs(question)]\n",
        "    result = qa_chain({\"input_documents\": similar_docs, \"question\": question})\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jUusXIsAcgE",
        "outputId": "41399fd2-a85c-4cf7-a58b-0e87efb8b85c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert about the Amadeus' DataIO framework.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using only textual descriptions and code examples taken from Data I/O documentation.\n",
            "\n",
            "    ['Data I/O\\n\\x7fHome\\n\\x7fGetting started\\n\\x7fMain concepts\\n\\x7fWriting your processors\\nConfiguration\\n\\x7fProcessors\\n\\x7fSchema definitions\\n\\x7fPaths\\n\\x7fDate filters\\nPipes\\n\\x7fFile system storage\\n\\x7fElasticsearch\\n\\x7fKafka\\nDistributors\\n\\x7fEmail\\nAdvanced\\n\\x7fCustom processor traits\\nAmadeus IT Group\\n\\x7fGitHub\\nMain Concepts\\nTable of contents\\nMain Concepts\\nBuilding Blocks\\n1.Pipeline\\n2.Pipes\\n3.Distributors\\n4.HandlerAccessor\\n5.Processors\\n2.Lifecycle of a Data I/O application\\nThis page provides an overview of the key aspects that form the foundation of the Data I/O framework. Understanding these concepts is essential for effectively\\nusing the framework to design and implement your data processing pipelines.\\nBuilding Blocks\\nPipeline\\nThe pipeline is the central component of the Data I/O framework. Its responsible for instantiating the components defined in the configuration and running the\\nprocessor. You can easily define and configure your pipeline using the provided configuration file, allowing you to focus on the essential task of data\\ntransformation.\\nFor more information about how to configure your applications, visit the configuration page.\\nPipes\\nPipes are the fundamental components of the Data I/O framework, representing the sources and destinations of data within the pipeline. Inputs handle the\\nreading of data from various sources, while outputs handle the writing of transformed data to different destinations. By configuring inputs and outputs in the\\nprovided configuration file, you can define the specific data sources and destinations for your ETL pipelines. Data I/O provides flexible and extensible options for\\nhandling various file formats, databases, and streaming data sources.\\nIn your code, pipes are closely related to Spark’s reading and writing capabilities. When configuring your application, you’ll specify the source or destination of\\nyour data, and Data I/O will use Spark’s underlying APIs to perform the actual reading and writing.\\nNo matter where data comes from or goes, each input pipe returns a DataFrame and each output pipe receives a DataFrame to write.\\nDistributors\\nDistributors in Data I/O are responsible for handling the distribution of files after the processing is complete. They provide methods and utilities to send\\nprocessed data to different destinations, such as email or other custom channels. By incorporating distributors into your pipeline, you can automate the\\ndistribution process, ensuring that the transformed data reaches the intended recipients or systems seamlessly. Data I/O offers a flexible and extensible\\narchitecture that allows you to define and configure distributors based on your specific distribution needs.\\n']\n",
            "\n",
            "    Question: Can you shortly describe me the Data I/O framework?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Data I/O framework is a framework for processing data in Apache Spark that was introduced by Amadeus in 2018. It provides a set of concepts and APIs to build complex data pipelines easily.\n",
            "\n",
            "The concept is that of a framework, which provides a centralized place for configurations and handlers, and provides a good abstraction of the actual details.\n",
            "\n",
            "The concepts include Pipeline, which is the main component of the framework, which is responsible for instantiating the handlers and running the handlers. Pipeline can be configured and defined using the provided configuration file.\n",
            "\n",
            "Pipes are the fundamental components of the Data I/O framework, representing the sources and destinations of data within the pipeline. Inputs handle the reading of data from various sources, and outputs handle the writing of transformed data to different destinations.\n",
            "\n",
            "Handlers are responsible for handling the distribution of files after the processing is completed. They provide methods and utilities to send processed data to different destinations, such as email or other custom channels. By incorporating handlers into your pipeline, you can automate the distribution process, ensuring that the transformed data reaches the intended recipients or systems seamlessly. The framework provides a flexible and extensible architecture that allows you to define and configure handlers based on your specific distribution needs.\n",
            "\n",
            "The Data I/O framework has changed a lot in the last couple of years. Please refer to the documentation to get familiar with the APIs.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CURL_CA_BUNDLE'] = ''\n",
        "\n",
        "\n",
        "question = \"Can you shortly describe me the Data I/O framework?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0kjS4E4NmpB",
        "outputId": "0321eeb7-7987-47c4-b95c-e32b89d71286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert about the Amadeus' DataIO framework.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using only textual descriptions and code examples taken from Data I/O documentation.\n",
            "\n",
            "    ['Data I/O\\n\\x7fHome\\n\\x7fGetting started\\n\\x7fMain concepts\\n\\x7fWriting your processors\\nConfiguration\\n\\x7fProcessors\\n\\x7fSchema definitions\\n\\x7fPaths\\n\\x7fDate filters\\nPipes\\n\\x7fFile system storage\\n\\x7fElasticsearch\\n\\x7fKafka\\nDistributors\\n\\x7fEmail\\nAdvanced\\n\\x7fCustom processor traits\\nAmadeus IT Group\\n\\x7fGitHub\\nData I/O\\nAutomated handling of inputs, outputs and files distribution through a unifed interface.\\nGet started View it on GitHub\\nWelcome to the Data I/O documentation!\\nData I/O is a Spark SQL Scala framework automating the process of reading, writing and distributing data. It gives you the tools to write ETL pipelines by focusing\\non the transformation of the data, regardless of its origin or destination.\\nBe sure to check out the getting started page for an overview of Data I/O’s features, or visit the main concepts page for a deeper dive.\\nIf you know your way around Data I/O and want to learn how to create your own pipes and distributors, head to the advanced section.\\n']\n",
            "\n",
            "    Question: In which programming language is the Data I/O framework written?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "The Data I/O framework is written in the language of Spark SQL Scala.\n"
          ]
        }
      ],
      "source": [
        "question = \"In which programming language is the Data I/O framework written?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D7OW1x5Ds9u",
        "outputId": "abb6df66-8fbf-4a0f-f545-2f8998bd69aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert about the Amadeus' DataIO framework.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using only textual descriptions and code examples taken from Data I/O documentation.\n",
            "\n",
            "    ['No streaming input is currently available for Elasticsearch in Data I/O.\\nOutput\\nType: com.amadeus.dataio.pipes.elk.streaming.ElkOutput\\n Name\\n Mandatory\\n Description\\n Example\\n Default\\n \\nDuration\\nYes\\nSets the trigger for the stream\\nquery. Controls the trigger()\\nSpark function.\\nDuration = \"60 seconds\"\\nTimeout\\nYes\\nControls the amount of time\\nbefore returning from the\\nstreaming query, in hours. It can\\nbe a String or an Int.\\nTimeout = 24\\nMode\\nYes\\nThe Spark Structured Streaming\\noutput mode.\\nMode = \"complete\"\\nappend\\n']\n",
            "\n",
            "    Question: Can Data I/O read data from Elasticsearch?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "No\n"
          ]
        }
      ],
      "source": [
        "question = \"Can Data I/O read data from Elasticsearch?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsQDo-gnLXb9",
        "outputId": "0cbf6553-9d27-4b58-bd61-f69ec15eb2f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert about the Amadeus' DataIO framework.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using only textual descriptions and code examples taken from Data I/O documentation.\n",
            "\n",
            "    ['Data I/O\\n\\x7fHome\\n\\x7fGetting started\\n\\x7fMain concepts\\n\\x7fWriting your processors\\nConfiguration\\n\\x7fProcessors\\n\\x7fSchema definitions\\n\\x7fPaths\\n\\x7fDate filters\\nPipes\\n\\x7fFile system storage\\n\\x7fElasticsearch\\n\\x7fKafka\\nDistributors\\n\\x7fEmail\\nAdvanced\\n\\x7fCustom processor traits\\nAmadeus IT Group\\n\\x7fGitHub\\n1.Configuration\\n2.Pipes\\n3.Elasticsearch\\nElasticsearch\\nAllows the connection to Elasticsearch nodes to automatically read and write data to an index in batch and streaming.\\nCode repository: https://github.com/AmadeusITGroup/dataio-framework/tree/main/src/main/scala/com/amadeus/dataio/pipes/elk\\nUseful links:\\n\\x7fJava DateTimeFormatter\\nCommon\\nThe following fields are available for all Elasticsearch components:\\n Name\\n Mandatory\\n Description\\n Example\\n Default\\n Index\\nYes\\nThe index where the data will be\\nwritten to.\\nDateField\\nYes\\nThe document date field to be\\nused for sub index partitioning.\\nDateField = \"docTimestamp\"\\nSubIndexDatePattern\\nNo\\nSet the date pattern to use while\\ncomputing the sub index suffix.\\nSee Java DateTimeFormatter\\ndocumentation.\\nSubIndexDatePattern =\\n\"yyyy.MM\"\\nyyyy.MM\\nOptions\\nYes\\nSpark options, as key = value\\npairs. \"es.nodes\" and \"es.port\"\\nare currently mandatory.\\nOptions { es.nodes =\\n\"elk.mycompany.com\", es.port =\\n\"9000\" }\\nBatch\\nNo batch input is currently available for Elasticsearch in Data I/O.\\nOutput\\nType: com.amadeus.dataio.pipes.elk.batch.ElkOutput\\n Name\\n Mandatory\\n Description\\n Example\\n Default\\n Mode\\nYes\\nThe Spark SQL write mode.\\nMode = \"overwrite\"\\nerror\\nStreaming\\n']\n",
            "\n",
            "    Question: Can Data I/O write data to Elasticsearch?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Yes. Data I/O provides the possibility to write data to Elasticsearch.\n"
          ]
        }
      ],
      "source": [
        "question = \"Can Data I/O write data to Elasticsearch?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8MAfADiGDff",
        "outputId": "3ba42718-4327-485a-a7ae-f88f27512c05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert about the Amadeus' DataIO framework.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using only textual descriptions and code examples taken from Data I/O documentation.\n",
            "\n",
            "    ['}\\nA same schema can be used by several inputs, but only one schema can be defined for a given input.\\nRegistering the schema\\nThe schemas must be registered via the SchemaRegistry, which centralizes all available schemas for the Data I/O inputs. To do so, call the\\nregisterSchema method within the main function of your application, as shown below:\\npackage myproject  \\nimport com.amadeus.dataio.{Pipeline, SchemaRegistry}\\nimport myproject.models.{ComplexQuery, Query}\\nobject MyApplicationPipeline extends App {\\n   val spark: SparkSession = (...)\\n   SchemaRegistry.registerSchema[Query]()\\n   SchemaRegistry.registerSchema[ComplexQuery]()\\n   val pipeline = Pipeline(\"/path/to/application.conf\")\\n   pipeline.run(spark)\\n}\\nAs you can see, registering several schemas is as simple as calling registerSchema for each schema you want to be registered.\\nAll the schemas defined in the configuration file must be registered before the call to the run function of your pipeline. If a schema is not found in the registry then\\nan IllegalArgumentException is thrown at runtime.\\n']\n",
            "\n",
            "    Question: How to add Amadeus Data I/O Maven dependency in my pom.xml?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "For adding Amadeus Data I/O Maven dependency in your pom.xml use the following syntax:\n",
            "\n",
            "Add the following to your <dependencies> section:\n",
            "  <dependency>\n",
            "    <groupId>com.amadeus</groupId>\n",
            "    <artifactId>dataio</artifactId>\n",
            "    <version>1.0.0</version>\n",
            "  </dependency>\n",
            "\n",
            "\n",
            "For more detailed information please refer to https://www.amadeus.com/docs/spark-mllib-features#add_a_maven_dependency.\n"
          ]
        }
      ],
      "source": [
        "question = \"How to add Amadeus Data I/O Maven dependency in my pom.xml?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlGTQwCOKLO_",
        "outputId": "4451e915-81e4-4992-aa33-a21dc6ed6ac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert about the Amadeus' DataIO framework.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using only textual descriptions and code examples taken from Data I/O documentation.\n",
            "\n",
            "    ['Data I/O\\n\\x7fHome\\n\\x7fGetting started\\n\\x7fMain concepts\\n\\x7fWriting your processors\\nConfiguration\\n\\x7fProcessors\\n\\x7fSchema definitions\\n\\x7fPaths\\n\\x7fDate filters\\nPipes\\n\\x7fFile system storage\\n\\x7fElasticsearch\\n\\x7fKafka\\nDistributors\\n\\x7fEmail\\nAdvanced\\n\\x7fCustom processor traits\\nAmadeus IT Group\\n\\x7fGitHub\\n1.Configuration\\n2.Distributors\\n3.Email\\nEmail\\nAllows to send files by email.\\nCode repository: https://github.com/AmadeusITGroup/dataio-framework/tree/main/src/main/scala/com/amadeus/dataio/distributors/email\\nUseful links:\\n\\x7fjavax.mail JavaDoc\\nFields\\nThe following fields are available for Email distributor components:\\n Name\\n Mandatory\\n Description\\n Example\\n Default\\n SmtpHost\\nYes\\nThe SMTP host to use to send\\nemails.\\nSmtpUser\\nYes\\nThe SMTP username to use to\\nauthenticate.\\nSmtpPassword\\nYes\\nThe SMTP password to use to\\nauthenticate.\\nFrom\\nYes\\nSpecifies the sender\\'s email\\naddress.\\nme@mycompany.com\\nTo\\nYes\\nSpecifies the recipient\\'s email\\naddress.\\n[\"john@smith.com\"]\\nCc\\nNo\\nAllows you to include additional\\nrecipients in the \"Cc\" (carbon\\ncopy) field of the email.\\n[\"john@smith.com\"]\\nBcc\\nNo\\nEnables the inclusion of\\nrecipients in the \"Bcc\" (blind\\ncarbon copy) field of the email.\\n[\"john@smith.com\"]\\nSubject\\nYes\\nSets the subject line of the\\nemail.\\nBody\\nNo\\nProvides the body content of the\\nemail.\\nCompressed\\nNo\\nA boolean flag indicating\\nwhether the email attachments\\nare compressed.\\ntrue\\nfalse\\n']\n",
            "\n",
            "    Question: List me the names of the mandatory field needed to send an Email with Data I/O?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Name\n",
            "Mandatory\n",
            "Description\n",
            "Example\n",
            "Default\n",
            "SmtpHost\n",
            "Yes\n",
            "The SMTP host to use to send emails.\n",
            "SmtpUser\n",
            "Yes\n",
            "The SMTP username to use to authenticate\n",
            "SmtpPassword\n",
            "Yes\n",
            "The SMTP password to use to authenticate.\n",
            "From\n",
            "Yes\n",
            "Specifies the sender's email address.\n",
            "me@mycompany.com\n",
            "To\n",
            "Yes\n",
            "Specifies the recipient's email address.\n",
            "[\"john@smith.com\"]\n",
            "Cc\n",
            "No\n",
            "Allows you to include additional recipients in the \"Cc\" (carbon copy) field of the email.\n",
            "[\"john@smith.com\"]\n",
            "Bcc\n",
            "No\n",
            "Enables the inclusion of recipients in the \"Bcc\" (blind carbon copy) field of the email.\n",
            "[\"john@smith.com\"]\n",
            "Subject\n",
            "Yes\n",
            "Sets the subject line of the email.\n",
            "Body\n",
            "No\n",
            "Provides the body content of the email.\n",
            "Compressed\n",
            "No\n",
            "A boolean flag indicating whether the email attachments are compressed. true\n",
            "false\n"
          ]
        }
      ],
      "source": [
        "question = \"List me the names of the mandatory field needed to send an Email with Data I/O?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Upj3dDvlMF_G",
        "outputId": "a4d46970-382c-4484-fe21-29f98202be1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert about the Amadeus' DataIO framework.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using only textual descriptions and code examples taken from Data I/O documentation.\n",
            "\n",
            "    ['Data I/O\\n\\x7fHome\\n\\x7fGetting started\\n\\x7fMain concepts\\n\\x7fWriting your processors\\nConfiguration\\n\\x7fProcessors\\n\\x7fSchema definitions\\n\\x7fPaths\\n\\x7fDate filters\\nPipes\\n\\x7fFile system storage\\n\\x7fElasticsearch\\n\\x7fKafka\\nDistributors\\n\\x7fEmail\\nAdvanced\\n\\x7fCustom processor traits\\nAmadeus IT Group\\n\\x7fGitHub\\n1.Configuration\\n2.Processors\\nConfiguring Your Processors\\nThe Data I/O framework allows you to configure and define your data processors in the application configuration file. This provides flexibility and ease of\\ncustomization by separating the processor implementation from the configuration details.\\nConfiguration Syntax\\nTo configure a processor, you need to define it in the application configuration file using the following syntax:\\nProcessing {\\n Type = \"gettingstarted.DuplicatesDropper\"\\n CustomValue1 = \"june\"\\n CustomValue2 = \"2023\"\\n}\\nThe naming convention of your custom configuration fields must follow the Typesafe naming conventions\\nTo see how to access these values in your processors code, see the dedicated page.\\n']\n",
            "\n",
            "    Question: List me the steps to create my own Amadeus Data I/O processor?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Step 1: Gather all the needed concepts and definitions\n",
            "\n",
            "Step 2: Create a processor configuration file\n",
            "\n",
            "Step 3: Implement your processor with your own custom code\n",
            "\n",
            "Step 4: Write a proper doc in the official documentation for your new processor\n"
          ]
        }
      ],
      "source": [
        "question = \"List me the steps to create my own Amadeus Data I/O processor?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4ypdxdSNViX",
        "outputId": "338cf8f7-dac7-404d-ca90-80ac2764604e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert about the Amadeus' DataIO framework.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using only textual descriptions and code examples taken from Data I/O documentation.\n",
            "\n",
            "    ['Data I/O\\n\\x7fHome\\n\\x7fGetting started\\n\\x7fMain concepts\\n\\x7fWriting your processors\\nConfiguration\\n\\x7fProcessors\\n\\x7fSchema definitions\\n\\x7fPaths\\n\\x7fDate filters\\nPipes\\n\\x7fFile system storage\\n\\x7fElasticsearch\\n\\x7fKafka\\nDistributors\\n\\x7fEmail\\nAdvanced\\n\\x7fCustom processor traits\\nAmadeus IT Group\\n\\x7fGitHub\\n1.Configuration\\n2.Pipes\\nPipes\\nPipes in the Data I/O framework are responsible for handling the reading and writing of data within your pipeline. They provide a convenient and configurable way\\nto interact with different data sources and destinations.\\nCommon Fields\\nAll pipes have access to at least two fields: Name and Type.\\n Name\\n Mandatory\\n Description\\n Example\\n Default\\n \\nName\\nNo\\nThe name of the pipe, that can\\nbe used to access it from the\\nHandlerAccessor.\\nName = \"my-input\"\\nType\\nYes\\nThe fully qualified name of the\\nclass to use as pipe (Input or\\nOutput).\\nType =\\n\"com.amadeus.dataio.pipes.storage.batch.StorageInput\"\\nAlthough the name is not mandatory, it is highly recommended to specify it unless you are using a specific type of processor that automatically fetches pipes by\\nindexes, rather than names (e.g. Transformer).\\nBatch and Streaming Inputs/Outputs\\nDue to how Spark operates, it’s important to note that the type of input and output you use should match when using the Data I/O framework. If you’re reading\\ndata from a batch source, such as a batch file or database, it’s necessary to use a batch output for writing the transformed data. Similarly, if you’re working with a\\nstreaming input, such as a streaming data source or a Kafka topic, then a streaming output is necessary for writing the processed data.\\nWhile it’s technically possible with Spark to mix batch and streaming inputs/outputs (for instance, using foreachBatch), it can lead to unexpected behavior or\\nissues. Therefore, it’s generally recommended to ensure that the type of input and output you use aligns with each other in your pipeline.\\nIn the following pages, you’ll find more detailed information about configuring specific types of inputs and outputs, along with examples and best practices.\\nTable of contents\\n\\x7fFile system storage\\n\\x7fElasticsearch\\n\\x7fKafka\\n']\n",
            "\n",
            "    Question: What are Pipes in Data I/O?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Pipes in the Data I/O framework are responsible for handling the reading and writing of data within your pipeline. They provide a convenient and configurable way to interact with different data sources and destinations.\n"
          ]
        }
      ],
      "source": [
        "question = \"What are Pipes in Data I/O?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q88rwGTNOUsX",
        "outputId": "bd96fee2-c32e-411e-faff-89664137ac3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert about the Amadeus' DataIO framework.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using only textual descriptions and code examples taken from Data I/O documentation.\n",
            "\n",
            "    ['Data I/O\\n\\x7fHome\\n\\x7fGetting started\\n\\x7fMain concepts\\n\\x7fWriting your processors\\nConfiguration\\n\\x7fProcessors\\n\\x7fSchema definitions\\n\\x7fPaths\\n\\x7fDate filters\\nPipes\\n\\x7fFile system storage\\n\\x7fElasticsearch\\n\\x7fKafka\\nDistributors\\n\\x7fEmail\\nAdvanced\\n\\x7fCustom processor traits\\nAmadeus IT Group\\n\\x7fGitHub\\n1.Configuration\\n2.Distributors\\nDistributors\\nData I/O introduces a feature known as Distributors. Distributors play a crucial role in the framework by facilitating the efficient transfer of files to their designated\\ndestinations, such as email addresses, remote servers, etc.\\nDistributors are configured within the Distribution section of your configuration. In this documentation, we will explore how to configure and utilize Distributors for\\nseamless file distribution.\\nCommon Fields\\nSimilarily to pipes, distributors have access to at least two fields: Name and Type.\\n Name\\n Mandatory\\n Description\\n Example\\n Default\\n \\nName\\nNo\\nThe name of the distributor, that\\ncan be used to access it from\\nthe HandlerAccessor.\\nName = \"email-sender\"\\nType\\nYes\\nThe fully qualified name of the\\nclass to use as distributor.\\nType =\\n\"com.amadeus.dataio.distributors.email.EmailDistributor\"\\nAlthough the name is not mandatory, it is highly recommended to specify it unless you are using a specific type of processor that automatically fetches distributors\\nby indexes, rather than names (although no such processor type currently exists by default).\\nTable of contents\\n\\x7fEmail\\n']\n",
            "\n",
            "    Question: What are Distributors in Data I/O?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Distributors are a key component of Data I/O framework that facilitates the efficient transfer of files to their designated destinations.\n"
          ]
        }
      ],
      "source": [
        "question = \"What are Distributors in Data I/O?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
