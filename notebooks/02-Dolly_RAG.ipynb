{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Wn41qFaX7W39",
      "metadata": {
        "id": "Wn41qFaX7W39"
      },
      "source": [
        "# Reading The Scala 2 Cookbook\n",
        "\n",
        "<img src=\"https://m.media-amazon.com/images/I/91AfDQsL7SL._AC_UF1000,1000_QL80_.jpg\" width=\"300\" />\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "x6QfpSq_0psO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6QfpSq_0psO",
        "outputId": "a2858ea6-14a3-422f-c1c6-2fe4de85a2ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 722/722 [00:09<00:00, 79.30it/s]\n"
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "\n",
        "def divide_chunks(text_body, min_chunk_size=300, max_chunk_size=1000):\n",
        "    in_chunks = text_body.split(\".\\n\")\n",
        "    out_chunks = []\n",
        "    curr_chunk = \"\"\n",
        "    for ic in in_chunks:\n",
        "        if len(ic) < min_chunk_size:\n",
        "            if len(ic)+len(curr_chunk) < max_chunk_size:\n",
        "                curr_chunk = curr_chunk + \"\\n\" + ic\n",
        "            else:\n",
        "                out_chunks.append(curr_chunk)\n",
        "                curr_chunk = \"\"\n",
        "    return out_chunks\n",
        "\n",
        "\n",
        "def clean_chunk(raw_chunk):\n",
        "    chunk = raw_chunk.replace('-\\n','').replace('\\n',' ')\n",
        "    return chunk\n",
        "\n",
        "\n",
        "body = \"\"\n",
        "with open(f\"data/pdf/scala_book/Scala_Cookbook.pdf\", \"rb\") as file:\n",
        "    reader = PyPDF2.PdfReader(file)\n",
        "    for page in tqdm(reader.pages):\n",
        "        text = page.extract_text()\n",
        "        body += \"\\n\\n\" + text\n",
        "    chunk_list = [clean_chunk(c) for c in divide_chunks(body)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VFg86ZxH7364",
      "metadata": {
        "id": "VFg86ZxH7364"
      },
      "source": [
        "# Embeddings\n",
        "##### Multidimensional Vector Representation of Semantic Meanings\n",
        "<img src=\"https://corpling.hypotheses.org/files/2018/04/Screen-Shot-2018-04-25-at-13.21.44.png\" width=\"400\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "-frzMG7i2JiZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-frzMG7i2JiZ",
        "outputId": "334b04be-2560-4a41-ed14-188a35364689"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 175949/175949 [37:47<00:00, 77.59it/s]\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import itertools\n",
        "\n",
        "model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
        "\n",
        "model = SentenceTransformer(model_name, device='cuda')\n",
        "\n",
        "sentences = list(itertools.chain(*chunk_list))\n",
        "\n",
        "embeddings = []\n",
        "for s in tqdm(sentences):\n",
        "  embeddings.append([float(x) for x in model.encode(s)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cNnVah5U77N7",
      "metadata": {
        "id": "cNnVah5U77N7"
      },
      "source": [
        "# Database\n",
        "##### Creating a Vector Database of Embeddings using the open-source ChromaDB\n",
        "<img src=\"https://www.mlq.ai/content/images/2023/08/1_admwyPyR6v_IZI0EYE--eA-1.webp\" width=\"250\" />\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "4YFIO78v3TK6",
      "metadata": {
        "id": "4YFIO78v3TK6"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
        "\n",
        "collection = chroma_client.get_or_create_collection(name=\"scala_book_chunks\", embedding_function=default_ef)\n",
        "\n",
        "MAX_CHUNKS = 41600\n",
        "\n",
        "collection.add(\n",
        "    documents=sentences[:MAX_CHUNKS],\n",
        "    embeddings=embeddings[:MAX_CHUNKS],\n",
        "    ids=[str(x) for x in range(MAX_CHUNKS)]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zmcmLLW48C-c",
      "metadata": {
        "id": "zmcmLLW48C-c"
      },
      "source": [
        "# ChatBot\n",
        "##### Using the open-source Databricks' Dolly model fine-tuned through the RAG techinique\n",
        "<img src=\"https://www.databricks.com/sites/default/files/2023-04/Dolly-logo.png\" width=\"300\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "iYVRZWe53Wm0",
      "metadata": {
        "id": "iYVRZWe53Wm0"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from langchain import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "\n",
        "def build_qa_chain():\n",
        "\n",
        "    model_name = \"databricks/dolly-v2-3b\" # Dolly smallest version (3 billion params)\n",
        "\n",
        "    instruct_pipeline = pipeline(model=model_name, torch_dtype=torch.bfloat16, trust_remote_code=True,\n",
        "                                 return_full_text=True, max_new_tokens=4096, top_p=0.95, top_k=50,\n",
        "                                 device=0) #cuda\n",
        "\n",
        "    template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "    Instruction:\n",
        "    You are an expert Scala developer.\n",
        "    You use a simple language to explain concepts.\n",
        "    You reply using only short textual descriptions and code expamples.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Response:\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(input_variables=['context', 'question'], template=template)\n",
        "\n",
        "    hf_pipe = HuggingFacePipeline(pipeline=instruct_pipeline)\n",
        "\n",
        "    return load_qa_chain(llm=hf_pipe, chain_type=\"stuff\", prompt=prompt, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mBE8PPgc3Z_3",
      "metadata": {
        "id": "mBE8PPgc3Z_3"
      },
      "outputs": [],
      "source": [
        "# Building the chain will load Dolly and can take several minutes depending on the model size\n",
        "qa_chain = build_qa_chain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "Z7QTT3203cW6",
      "metadata": {
        "id": "Z7QTT3203cW6"
      },
      "outputs": [],
      "source": [
        "class Document():\n",
        "    def __init__(self, content):\n",
        "        self.page_content = content\n",
        "        self.metadata = {\"metadata\": \"\"}\n",
        "\n",
        "def get_similar_docs(question):\n",
        "    results = collection.query(\n",
        "        query_embeddings=[float(x) for x in model.encode(question)],\n",
        "        n_results=3\n",
        "    )\n",
        "    return results[\"documents\"]\n",
        "\n",
        "def answer_question(question):\n",
        "    similar_docs = [Document(x) for x in get_similar_docs(question)]\n",
        "    result = qa_chain({\"input_documents\": similar_docs, \"question\": question})\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "r98Qn6sc3f5h",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r98Qn6sc3f5h",
        "outputId": "fff2034b-703a-4c76-bc19-88f86338a1c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert Scala developer.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using only short textual descriptions and code expamples.\n",
            "\n",
            "    ['s', 's', 's']\n",
            "\n",
            "    Question: Which are the strenghts of the Scala programming language?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "The following are the strenghts of the Scala programming language:\n",
            "\n",
            "- static typing, meaning that the type of an expression is fully determined at compile time\n",
            "- strong type inference, meaning that type information is used to perform more efficient runtime checks\n",
            "- functional programming style, meaning that imperative constructs are not required\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CURL_CA_BUNDLE'] = ''\n",
        "\n",
        "question = \"Which are the strenghts of the Scala programming language?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "9MlPRjx5Ifeq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MlPRjx5Ifeq",
        "outputId": "e130db57-38b4-4ccb-ad7a-ef6fb9f60509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert Scala developer.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using only short textual descriptions and code expamples.\n",
            "\n",
            "    ['+', '+', '+']\n",
            "\n",
            "    Question: What are side effects?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Side effects are an operations aspect of programming languages that can influence the result of an operation, especially one that writes data to a system's file system.\n",
            "\n",
            "To some extent, the distinction between \"side effects\" and other effects is semantic rather than a matter of whether one should count something as a side effect or not, so it is useful to talk about both.\n"
          ]
        }
      ],
      "source": [
        "question = \"What are side effects?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "XxvbUYsAI9WJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxvbUYsAI9WJ",
        "outputId": "7c6d3b45-51b3-4b79-ccf1-4caa68b7e6c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "    Instruction:\n",
            "    You are an expert Scala developer.\n",
            "    You use a simple language to explain concepts.\n",
            "    You reply using only short textual descriptions and code expamples.\n",
            "\n",
            "    ['s', 's', 's']\n",
            "\n",
            "    Question: How do I transform a sequence of string to uppercase?\n",
            "\n",
            "    Response:\n",
            "    \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Short textual description:\n",
            "\n",
            "You can use the toUpper method of the String class.\n",
            "\n",
            "Example:\n",
            "\n",
            "scala> 'hello'.toUpper\n",
            "res0: String = HELLO\n",
            "\n",
            "Longer textual description:\n",
            "\n",
            "You can use the toUpper method of the String class.\n",
            "\n",
            "It applies a global transformation to each of its characters.\n",
            "\n",
            "The toUpper method transforms a string into a new string that contains all the characters of the string,\n",
            "but with each of them transformed in upper case.\n"
          ]
        }
      ],
      "source": [
        "question = \"How do I transform a sequence of string to uppercase?\"\n",
        "\n",
        "answer = answer_question(question)\n",
        "\n",
        "print(answer[\"output_text\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
